{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_TH7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGFBRJdd05ug",
        "colab_type": "text"
      },
      "source": [
        "Về cơ bản, Tensorflow là một thư viện trên python, hỗ trợ các tính toán tensor (block ma trận) nhanh, phù hợp với các bài toán tính toán ma trận nhiều như deep learning.\n",
        "\n",
        "Để lập trình toán tensorflow gồm 2 bước:\n",
        "    1. Lập trình một đồ thị tính toán: trong bước này, chúng ta phải xây dựng một graph (đồ thị) có nhiệm vụ chỉ dẫn các phép toán sẽ lần lượt được chạy như thế nào.\n",
        "    2. Truyền data vào và tính toán.\n",
        "    \n",
        "Ví dụ dưới sẽ hưỡng dẫn lập trình phép tính nhân 2 ma trận 2x2, với 2 ma trận sẽ được truyền vào lúc tính."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7FEFHtfKOue",
        "colab_type": "code",
        "outputId": "aa0028cd-8d96-40cc-e549-6fdc96a0651d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "path=\"gdrive/My Drive/DATASCIENCE/THML/\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoLxPMVv05uh",
        "colab_type": "text"
      },
      "source": [
        "## Bước xây dựng graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ot1F84gR05ui",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        },
        "outputId": "de632bc8-2c4b-43ca-dc3e-07ff12a019d6"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "A_matrix = tf.placeholder(shape=[2,2], dtype=tf.float32)\n",
        "B_matrix = tf.placeholder(shape=[2,2], dtype=tf.float32)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqVbWEUH05ul",
        "colab_type": "text"
      },
      "source": [
        "placeholder là một chỗ chứa để mình truyền data vào trong lúc tính toán.\n",
        "\n",
        "    shape: chiều của ma trận truyền vào\n",
        "    \n",
        "    dtype: kiểu số"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bBjADR_05um",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_matrix = tf.matmul(A_matrix, B_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgDD4DA005up",
        "colab_type": "text"
      },
      "source": [
        "hàm matmul sẽ tạo một node trong graph, node này có nhiệm vụ nhân 2 ma trận với nhau.\n",
        "\n",
        "Như vậy là xong bước tạo graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys876OMu05up",
        "colab_type": "text"
      },
      "source": [
        "## Truyền data vào và tính"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtZ1avPu05uq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "session = tf.Session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jw9U-_dC05us",
        "colab_type": "text"
      },
      "source": [
        "Tensorflow sẽ hoạt động theo từng session (phiên làm việc), \n",
        "nhiều session có thể dùng cùng một graph để tính toán (mặc định là dùng default graph)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxsOZFh-05ut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "A = np.array([[1,2],[3,4]])\n",
        "B = np.array([[5,6],[7,8]])\n",
        "#Giả sử 2 ma trận A và B bên trên là data ta muốn truyền vào để tính toán\n",
        "\n",
        "\n",
        "feed_dict = {A_matrix: A, B_matrix: B}\n",
        "#dòng trên có nhiệm vụ khai báo xem data nào sẽ được truyền vào placeholder nào"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXFEdkrq05uv",
        "colab_type": "code",
        "outputId": "7387d6e3-f314-44ac-b475-80db795f0d25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "C = session.run((result_matrix), feed_dict=feed_dict)\n",
        "print(C)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[19. 22.]\n",
            " [43. 50.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BGHefVR05uy",
        "colab_type": "text"
      },
      "source": [
        "hàm run trên có nhiệm vụ thực hiện tính toán, với feed_dict đã được khai báo ở dòng trước\n",
        "hàm Session.run(tensor, feed_dict)\n",
        "\n",
        "    tensor: là node trong graph mình muốn thực hiện tính toán và trả về\n",
        "    \n",
        "    feed_dict: kiểu dict, có nhiệm vụ khai báo data nào truyền vào placeholder nào"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S4l0nWk05uy",
        "colab_type": "text"
      },
      "source": [
        "Tiếp theo, chúng ta sẽ thực nghiệm với  bài toán Linear Regression đã làm ở bài thực hành một với Gradient Descent của Tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "955xuKdx05uz",
        "colab_type": "code",
        "outputId": "9036aec8-d8ce-4e5e-8145-8a4fd9bab0a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(path+\"english_score.csv\")\n",
        "data.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TOEFL Score</th>\n",
              "      <th>GRE Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>118</td>\n",
              "      <td>337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>107</td>\n",
              "      <td>324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>104</td>\n",
              "      <td>316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>110</td>\n",
              "      <td>322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>103</td>\n",
              "      <td>314</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   TOEFL Score  GRE Score\n",
              "0          118        337\n",
              "1          107        324\n",
              "2          104        316\n",
              "3          110        322\n",
              "4          103        314"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBM-9wJ305u2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data[\"TOEFL Score\"].values.reshape(-1,1)\n",
        "y = data[\"GRE Score\"].values.reshape(-1,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUw-EgIv05vA",
        "colab_type": "code",
        "outputId": "50dc1464-727f-45a8-db9f-d46d62efe8a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Tạo graph\n",
        "\n",
        "\"\"\"\n",
        "Ta đang muốn dự đoán y khi đã biết X.\n",
        "\n",
        "Đầu tiên phải tạo 2 placeholder để chứa X và y.\n",
        "\"\"\"\n",
        "\n",
        "X_holder = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
        "y_holder = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
        "\n",
        "\"\"\"\n",
        "X_holder có nhiệm vụ nhận điểm TOEFL Score vào graph\n",
        "y_holder có nhiệm vụ nhận điểm GRE Score đúng, sử dụng như label trong quá trình train\n",
        "\n",
        "None trong shape thể hiện số dòng của data không xác định trước khi tính toán.\n",
        "\"\"\"\n",
        "\n",
        "theta_0 = tf.Variable(0.0, dtype=tf.float32)\n",
        "theta_1 = tf.Variable(0.0, dtype=tf.float32)\n",
        "\n",
        "\"\"\"\n",
        "Khai báo theta_0 và theta_1 như Variable, tức là các biến này sẽ được thay đổi, được \"học\" sao cho loss tốt nhất\n",
        "\n",
        " tf.Variable(init_value, dtype)\n",
        "     init_value: giá trị khởi tạo\n",
        "     dtype: kiểu dữ liệu\n",
        "\"\"\"\n",
        "\n",
        "y_hat = X_holder*theta_1 + theta_0\n",
        "\n",
        "\"\"\"\n",
        "y_hat là sẽ y mình dự đoán, các phép tính * và + trên được tính theo từng dòng, y_hat sẽ có shape [None, 1]\n",
        "\"\"\"\n",
        "\n",
        "loss = tf.square(y_hat - y_holder)\n",
        "\"\"\"\n",
        "ở dòng trên ta thực hiện tính loss theo từng dòng, phép - là phép tính trên từng dòng,\n",
        "tf.square sẽ bình phương từng phẩn tử\n",
        "\"\"\"\n",
        "\n",
        "average_loss = tf.reduce_mean(loss)\n",
        "\n",
        "\"\"\"\n",
        "dòng trên thực hiện tính loss trung bình trên toàn bộ data, shape của average_loss là [1]\n",
        "\n",
        "average_loss chính là loss chúng ta cần minimize\n",
        "\"\"\"\n",
        "\n",
        "train_step = tf.train.AdamOptimizer(learning_rate=0.005).minimize(average_loss)\n",
        "\n",
        "\"\"\"\n",
        "dòng trên sẽ tạo một node trên graph, node này có nhiệm vụ thực hiện \"train\", \n",
        "tức là sẽ thay đổi theta_0 và theta_1 sao cho minimize được node average_loss.\n",
        "\n",
        "AdamOptimizer là biến thể của Gradient Descent trong đó learning rate sẽ tự động được điều chỉnh\n",
        "learning_rate: là tốc độ học khởi tạo theo thuật toán\n",
        "\"\"\"\n",
        "print()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhSc3M8K05v5",
        "colab_type": "code",
        "outputId": "74f6ee6c-c770-4c73-f80e-415b5a2fc716",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#chạy\n",
        "\n",
        "sess = tf.Session()\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "#khởi tạo tất cả các biến trong graph trong session\n",
        "\n",
        "for i in range(47000):\n",
        "    sess.run((train_step), feed_dict={X_holder: X, y_holder: y})\n",
        "    #Chạy bước training\n",
        "    \n",
        "    if i % 100 == 0:\n",
        "        cur_loss = sess.run((average_loss), feed_dict={X_holder: X, y_holder: y})\n",
        "        print(\"Current loss: \", cur_loss)\n",
        "        #Tính xem loss hiện tạo là bao nhiêu\n",
        "        \n",
        "a, b = sess.run((theta_1, theta_0))\n",
        "print(a,b)\n",
        "#Do theta_0 và theta_1 không phụ thuộc vào placeholder, nên không cần phải khai bào data truyền vào"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current loss:  99939.19\n",
            "Current loss:  69526.37\n",
            "Current loss:  46690.13\n",
            "Current loss:  30167.951\n",
            "Current loss:  18678.688\n",
            "Current loss:  11041.693\n",
            "Current loss:  6215.9243\n",
            "Current loss:  3332.3445\n",
            "Current loss:  1710.78\n",
            "Current loss:  856.1906\n",
            "Current loss:  435.60718\n",
            "Current loss:  242.88438\n",
            "Current loss:  160.88052\n",
            "Current loss:  128.55905\n",
            "Current loss:  116.7883\n",
            "Current loss:  112.83821\n",
            "Current loss:  111.619865\n",
            "Current loss:  111.27424\n",
            "Current loss:  111.18335\n",
            "Current loss:  111.15975\n",
            "Current loss:  111.15222\n",
            "Current loss:  111.14796\n",
            "Current loss:  111.144226\n",
            "Current loss:  111.140396\n",
            "Current loss:  111.13632\n",
            "Current loss:  111.13205\n",
            "Current loss:  111.127525\n",
            "Current loss:  111.12275\n",
            "Current loss:  111.117714\n",
            "Current loss:  111.11242\n",
            "Current loss:  111.10688\n",
            "Current loss:  111.101\n",
            "Current loss:  111.0948\n",
            "Current loss:  111.08825\n",
            "Current loss:  111.08135\n",
            "Current loss:  111.074104\n",
            "Current loss:  111.0665\n",
            "Current loss:  111.058464\n",
            "Current loss:  111.050026\n",
            "Current loss:  111.04113\n",
            "Current loss:  111.03179\n",
            "Current loss:  111.02191\n",
            "Current loss:  111.011604\n",
            "Current loss:  111.00071\n",
            "Current loss:  110.989265\n",
            "Current loss:  110.97717\n",
            "Current loss:  110.96448\n",
            "Current loss:  110.95119\n",
            "Current loss:  110.93719\n",
            "Current loss:  110.92248\n",
            "Current loss:  110.90697\n",
            "Current loss:  110.89071\n",
            "Current loss:  110.87355\n",
            "Current loss:  110.855576\n",
            "Current loss:  110.83669\n",
            "Current loss:  110.81679\n",
            "Current loss:  110.79586\n",
            "Current loss:  110.773865\n",
            "Current loss:  110.75083\n",
            "Current loss:  110.726555\n",
            "Current loss:  110.70108\n",
            "Current loss:  110.67425\n",
            "Current loss:  110.64612\n",
            "Current loss:  110.61646\n",
            "Current loss:  110.58541\n",
            "Current loss:  110.55278\n",
            "Current loss:  110.51846\n",
            "Current loss:  110.48239\n",
            "Current loss:  110.44448\n",
            "Current loss:  110.404724\n",
            "Current loss:  110.36288\n",
            "Current loss:  110.319084\n",
            "Current loss:  110.27297\n",
            "Current loss:  110.22456\n",
            "Current loss:  110.17373\n",
            "Current loss:  110.12043\n",
            "Current loss:  110.064316\n",
            "Current loss:  110.00555\n",
            "Current loss:  109.94388\n",
            "Current loss:  109.87913\n",
            "Current loss:  109.811165\n",
            "Current loss:  109.73985\n",
            "Current loss:  109.66509\n",
            "Current loss:  109.586685\n",
            "Current loss:  109.50438\n",
            "Current loss:  109.41821\n",
            "Current loss:  109.32781\n",
            "Current loss:  109.23314\n",
            "Current loss:  109.13394\n",
            "Current loss:  109.02999\n",
            "Current loss:  108.92121\n",
            "Current loss:  108.80734\n",
            "Current loss:  108.688225\n",
            "Current loss:  108.563644\n",
            "Current loss:  108.433365\n",
            "Current loss:  108.29722\n",
            "Current loss:  108.155014\n",
            "Current loss:  108.006546\n",
            "Current loss:  107.85165\n",
            "Current loss:  107.690094\n",
            "Current loss:  107.52173\n",
            "Current loss:  107.34637\n",
            "Current loss:  107.16383\n",
            "Current loss:  106.974014\n",
            "Current loss:  106.77674\n",
            "Current loss:  106.571945\n",
            "Current loss:  106.35948\n",
            "Current loss:  106.13923\n",
            "Current loss:  105.91122\n",
            "Current loss:  105.67533\n",
            "Current loss:  105.43164\n",
            "Current loss:  105.18012\n",
            "Current loss:  104.92073\n",
            "Current loss:  104.653656\n",
            "Current loss:  104.37885\n",
            "Current loss:  104.09666\n",
            "Current loss:  103.80709\n",
            "Current loss:  103.51036\n",
            "Current loss:  103.20666\n",
            "Current loss:  102.896225\n",
            "Current loss:  102.57932\n",
            "Current loss:  102.256195\n",
            "Current loss:  101.9271\n",
            "Current loss:  101.5924\n",
            "Current loss:  101.25237\n",
            "Current loss:  100.907364\n",
            "Current loss:  100.55774\n",
            "Current loss:  100.20369\n",
            "Current loss:  99.84566\n",
            "Current loss:  99.48395\n",
            "Current loss:  99.118866\n",
            "Current loss:  98.750694\n",
            "Current loss:  98.379814\n",
            "Current loss:  98.00645\n",
            "Current loss:  97.63088\n",
            "Current loss:  97.25341\n",
            "Current loss:  96.87425\n",
            "Current loss:  96.49375\n",
            "Current loss:  96.111984\n",
            "Current loss:  95.7293\n",
            "Current loss:  95.34585\n",
            "Current loss:  94.96182\n",
            "Current loss:  94.577354\n",
            "Current loss:  94.19268\n",
            "Current loss:  93.80787\n",
            "Current loss:  93.42322\n",
            "Current loss:  93.03866\n",
            "Current loss:  92.6545\n",
            "Current loss:  92.27062\n",
            "Current loss:  91.88735\n",
            "Current loss:  91.5047\n",
            "Current loss:  91.12268\n",
            "Current loss:  90.74144\n",
            "Current loss:  90.361084\n",
            "Current loss:  89.98163\n",
            "Current loss:  89.60313\n",
            "Current loss:  89.22563\n",
            "Current loss:  88.849205\n",
            "Current loss:  88.47393\n",
            "Current loss:  88.09978\n",
            "Current loss:  87.726814\n",
            "Current loss:  87.35511\n",
            "Current loss:  86.98461\n",
            "Current loss:  86.61539\n",
            "Current loss:  86.247505\n",
            "Current loss:  85.88092\n",
            "Current loss:  85.51563\n",
            "Current loss:  85.15189\n",
            "Current loss:  84.78934\n",
            "Current loss:  84.42826\n",
            "Current loss:  84.06864\n",
            "Current loss:  83.71051\n",
            "Current loss:  83.35381\n",
            "Current loss:  82.9983\n",
            "Current loss:  82.6443\n",
            "Current loss:  82.29175\n",
            "Current loss:  81.94069\n",
            "Current loss:  81.59107\n",
            "Current loss:  81.24293\n",
            "Current loss:  80.89628\n",
            "Current loss:  80.551346\n",
            "Current loss:  80.207924\n",
            "Current loss:  79.865875\n",
            "Current loss:  79.5254\n",
            "Current loss:  79.1863\n",
            "Current loss:  78.848724\n",
            "Current loss:  78.51261\n",
            "Current loss:  78.17801\n",
            "Current loss:  77.84504\n",
            "Current loss:  77.51361\n",
            "Current loss:  77.18374\n",
            "Current loss:  76.855286\n",
            "Current loss:  76.52841\n",
            "Current loss:  76.20537\n",
            "Current loss:  75.87932\n",
            "Current loss:  75.557106\n",
            "Current loss:  75.236404\n",
            "Current loss:  74.91717\n",
            "Current loss:  74.59954\n",
            "Current loss:  74.283424\n",
            "Current loss:  73.96975\n",
            "Current loss:  73.65581\n",
            "Current loss:  73.34523\n",
            "Current loss:  73.03435\n",
            "Current loss:  72.725815\n",
            "Current loss:  72.41904\n",
            "Current loss:  72.11345\n",
            "Current loss:  71.809616\n",
            "Current loss:  71.507195\n",
            "Current loss:  71.206345\n",
            "Current loss:  70.90702\n",
            "Current loss:  70.60921\n",
            "Current loss:  70.31304\n",
            "Current loss:  70.018295\n",
            "Current loss:  69.72497\n",
            "Current loss:  69.43325\n",
            "Current loss:  69.14789\n",
            "Current loss:  68.854416\n",
            "Current loss:  68.57164\n",
            "Current loss:  68.28157\n",
            "Current loss:  67.99894\n",
            "Current loss:  67.714874\n",
            "Current loss:  67.43407\n",
            "Current loss:  67.15425\n",
            "Current loss:  66.876144\n",
            "Current loss:  66.59959\n",
            "Current loss:  66.32502\n",
            "Current loss:  66.05106\n",
            "Current loss:  65.77908\n",
            "Current loss:  65.51062\n",
            "Current loss:  65.23957\n",
            "Current loss:  64.97212\n",
            "Current loss:  64.706154\n",
            "Current loss:  64.44177\n",
            "Current loss:  64.179115\n",
            "Current loss:  63.917492\n",
            "Current loss:  63.657696\n",
            "Current loss:  63.399094\n",
            "Current loss:  63.142376\n",
            "Current loss:  62.887005\n",
            "Current loss:  62.632988\n",
            "Current loss:  62.380665\n",
            "Current loss:  62.129677\n",
            "Current loss:  61.880318\n",
            "Current loss:  61.632652\n",
            "Current loss:  61.386147\n",
            "Current loss:  61.141315\n",
            "Current loss:  60.89889\n",
            "Current loss:  60.65614\n",
            "Current loss:  60.415886\n",
            "Current loss:  60.17704\n",
            "Current loss:  59.94039\n",
            "Current loss:  59.70384\n",
            "Current loss:  59.4699\n",
            "Current loss:  59.23684\n",
            "Current loss:  59.005943\n",
            "Current loss:  58.775673\n",
            "Current loss:  58.547497\n",
            "Current loss:  58.320763\n",
            "Current loss:  58.095398\n",
            "Current loss:  57.87151\n",
            "Current loss:  57.64968\n",
            "Current loss:  57.428314\n",
            "Current loss:  57.209023\n",
            "Current loss:  56.99566\n",
            "Current loss:  56.77473\n",
            "Current loss:  56.559982\n",
            "Current loss:  56.346626\n",
            "Current loss:  56.137707\n",
            "Current loss:  55.924313\n",
            "Current loss:  55.715733\n",
            "Current loss:  55.507946\n",
            "Current loss:  55.301994\n",
            "Current loss:  55.09763\n",
            "Current loss:  54.894634\n",
            "Current loss:  54.69832\n",
            "Current loss:  54.493183\n",
            "Current loss:  54.294804\n",
            "Current loss:  54.09773\n",
            "Current loss:  53.902164\n",
            "Current loss:  53.708084\n",
            "Current loss:  53.515522\n",
            "Current loss:  53.324425\n",
            "Current loss:  53.13488\n",
            "Current loss:  52.94679\n",
            "Current loss:  52.760048\n",
            "Current loss:  52.57483\n",
            "Current loss:  52.391083\n",
            "Current loss:  52.20889\n",
            "Current loss:  52.02826\n",
            "Current loss:  51.848793\n",
            "Current loss:  51.67118\n",
            "Current loss:  51.49466\n",
            "Current loss:  51.320217\n",
            "Current loss:  51.14641\n",
            "Current loss:  50.977642\n",
            "Current loss:  50.80402\n",
            "Current loss:  50.635086\n",
            "Current loss:  50.467422\n",
            "Current loss:  50.301327\n",
            "Current loss:  50.13673\n",
            "Current loss:  49.97359\n",
            "Current loss:  49.814247\n",
            "Current loss:  49.651592\n",
            "Current loss:  49.49278\n",
            "Current loss:  49.33925\n",
            "Current loss:  49.17958\n",
            "Current loss:  49.02523\n",
            "Current loss:  48.872135\n",
            "Current loss:  48.720615\n",
            "Current loss:  48.57073\n",
            "Current loss:  48.421967\n",
            "Current loss:  48.27484\n",
            "Current loss:  48.12904\n",
            "Current loss:  47.98477\n",
            "Current loss:  47.84201\n",
            "Current loss:  47.7005\n",
            "Current loss:  47.56071\n",
            "Current loss:  47.42202\n",
            "Current loss:  47.284973\n",
            "Current loss:  47.14935\n",
            "Current loss:  47.015266\n",
            "Current loss:  46.882305\n",
            "Current loss:  46.750988\n",
            "Current loss:  46.621075\n",
            "Current loss:  46.49552\n",
            "Current loss:  46.365574\n",
            "Current loss:  46.241447\n",
            "Current loss:  46.11574\n",
            "Current loss:  45.994915\n",
            "Current loss:  45.87161\n",
            "Current loss:  45.75622\n",
            "Current loss:  45.633064\n",
            "Current loss:  45.517296\n",
            "Current loss:  45.400272\n",
            "Current loss:  45.287746\n",
            "Current loss:  45.173107\n",
            "Current loss:  45.065166\n",
            "Current loss:  44.951458\n",
            "Current loss:  44.843567\n",
            "Current loss:  44.735535\n",
            "Current loss:  44.629654\n",
            "Current loss:  44.52519\n",
            "Current loss:  44.422024\n",
            "Current loss:  44.320328\n",
            "Current loss:  44.220013\n",
            "Current loss:  44.12105\n",
            "Current loss:  44.02351\n",
            "Current loss:  43.927376\n",
            "Current loss:  43.832592\n",
            "Current loss:  43.739162\n",
            "Current loss:  43.647125\n",
            "Current loss:  43.556458\n",
            "Current loss:  43.467148\n",
            "Current loss:  43.37937\n",
            "Current loss:  43.292583\n",
            "Current loss:  43.207348\n",
            "Current loss:  43.123425\n",
            "Current loss:  43.0409\n",
            "Current loss:  42.95969\n",
            "Current loss:  42.879818\n",
            "Current loss:  42.80126\n",
            "Current loss:  42.724125\n",
            "Current loss:  42.64829\n",
            "Current loss:  42.57396\n",
            "Current loss:  42.50056\n",
            "Current loss:  42.42946\n",
            "Current loss:  42.35816\n",
            "Current loss:  42.2889\n",
            "Current loss:  42.220966\n",
            "Current loss:  42.15428\n",
            "Current loss:  42.088943\n",
            "Current loss:  42.02494\n",
            "Current loss:  41.962154\n",
            "Current loss:  41.900673\n",
            "Current loss:  41.8406\n",
            "Current loss:  41.781548\n",
            "Current loss:  41.72619\n",
            "Current loss:  41.667526\n",
            "Current loss:  41.612396\n",
            "Current loss:  41.558537\n",
            "Current loss:  41.505943\n",
            "Current loss:  41.4545\n",
            "Current loss:  41.404366\n",
            "Current loss:  41.35542\n",
            "Current loss:  41.307693\n",
            "Current loss:  41.261166\n",
            "Current loss:  41.216198\n",
            "Current loss:  41.171734\n",
            "Current loss:  41.131344\n",
            "Current loss:  41.086964\n",
            "Current loss:  41.046368\n",
            "Current loss:  41.00692\n",
            "Current loss:  40.968674\n",
            "Current loss:  40.93452\n",
            "Current loss:  40.89552\n",
            "Current loss:  40.860725\n",
            "Current loss:  40.8269\n",
            "Current loss:  40.79426\n",
            "Current loss:  40.763866\n",
            "Current loss:  40.732243\n",
            "Current loss:  40.702976\n",
            "Current loss:  40.67447\n",
            "Current loss:  40.6472\n",
            "Current loss:  40.62105\n",
            "Current loss:  40.595673\n",
            "Current loss:  40.57145\n",
            "Current loss:  40.54853\n",
            "Current loss:  40.52602\n",
            "Current loss:  40.504757\n",
            "Current loss:  40.48442\n",
            "Current loss:  40.465103\n",
            "Current loss:  40.446545\n",
            "Current loss:  40.428986\n",
            "Current loss:  40.412224\n",
            "Current loss:  40.3964\n",
            "Current loss:  40.381363\n",
            "Current loss:  40.36717\n",
            "Current loss:  40.359966\n",
            "Current loss:  40.341213\n",
            "Current loss:  40.32936\n",
            "Current loss:  40.31826\n",
            "Current loss:  40.30783\n",
            "Current loss:  40.29813\n",
            "Current loss:  40.28913\n",
            "Current loss:  40.2807\n",
            "Current loss:  40.278877\n",
            "Current loss:  40.265717\n",
            "Current loss:  40.259064\n",
            "Current loss:  40.25297\n",
            "Current loss:  40.247353\n",
            "Current loss:  40.242256\n",
            "Current loss:  40.24014\n",
            "Current loss:  40.23336\n",
            "Current loss:  40.22953\n",
            "Current loss:  40.226112\n",
            "Current loss:  40.223015\n",
            "Current loss:  40.221226\n",
            "Current loss:  40.217777\n",
            "Current loss:  40.215633\n",
            "Current loss:  40.213688\n",
            "Current loss:  40.211945\n",
            "Current loss:  40.21049\n",
            "Current loss:  40.209152\n",
            "Current loss:  40.208046\n",
            "Current loss:  40.20707\n",
            "Current loss:  40.2062\n",
            "Current loss:  40.20592\n",
            "Current loss:  40.204884\n",
            "Current loss:  40.20745\n",
            "Current loss:  40.203896\n",
            "Current loss:  40.20351\n",
            "Current loss:  40.203182\n",
            "Current loss:  40.206676\n",
            "Current loss:  40.202698\n",
            "Current loss:  40.202526\n",
            "Current loss:  40.202328\n",
            "Current loss:  40.204193\n",
            "Current loss:  40.202095\n",
            "Current loss:  40.202007\n",
            "Current loss:  40.20194\n",
            "Current loss:  40.201878\n",
            "Current loss:  40.202286\n",
            "Current loss:  40.201782\n",
            "Current loss:  40.201748\n",
            "Current loss:  40.201725\n",
            "Current loss:  40.201702\n",
            "Current loss:  40.20562\n",
            "Current loss:  40.201664\n",
            "Current loss:  40.20165\n",
            "1.537324 151.68307\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5zdm2sh05v7",
        "colab_type": "code",
        "outputId": "4fb4168a-f352-4b56-a702-a89e6c50a4db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "#Vẽ a, b dự đoán được\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(X, y)\n",
        "xx = np.linspace(90, 120)\n",
        "plt.plot(xx, xx*a+b)\n",
        "plt.xlabel('TOEFL Score')\n",
        "plt.ylabel('GRE Score')\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBcMwDKMOMwqGYRhGHf8fP9bvCpEAM0oAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3TRvNT_05v-",
        "colab_type": "text"
      },
      "source": [
        "# Bài tập"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yaol7FV705v_",
        "colab_type": "text"
      },
      "source": [
        "1. Hãy điều chỉnh Learning Rate và số vòng lặp sao cho nó hội tụ nhanh và tốt hơn.\n",
        "\n",
        "Vẽ kết quả chạy được\n",
        "\n",
        "---\n",
        "\n",
        "learning rate: 0,005\n",
        "số vòng lặp: 47000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUqU7N5N05v_",
        "colab_type": "text"
      },
      "source": [
        "2. Hãy chạy node y_hat với data truyền vào là mảng test_X ở dưới để đoán xem nhưng người có điểm TOEFL như phía dưới được bao nhiều điểm GRE\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcYdU2ry05wA",
        "colab_type": "code",
        "outputId": "ae27e2a2-b377-4f2c-b46f-c3824c212757",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "test_X = np.array([[95],[100],[102.5],[110],[117.1]])\n",
        "for i in range(5):\n",
        "  test_y =  test_X[i]*a + b\n",
        "  print(test_y)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[297.72885036]\n",
            "[305.41547012]\n",
            "[309.25878]\n",
            "[320.78870964]\n",
            "[331.7037097]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsXikhwB05wC",
        "colab_type": "text"
      },
      "source": [
        "3. Như ta đã biết Linear Regression có nghiệm chính xác khi giải bằng đại số là $(X^T*X)^{-1}*X^T*y$\n",
        "\n",
        "Trong đó:\n",
        "\n",
        "    $X$: là ma trận data có thêm một cột toàn số 1\n",
        "    \n",
        "    $X^T$: là chuyển vị của $X$\n",
        "    \n",
        "    $X^{-1}$: là ma trận nghịch đảo của $X$\n",
        "    \n",
        "    $y $là vector cột\n",
        "    \n",
        "    phép * đều là phép nhân ma trận (không phải nhân từng phẩn tử khi lập trình)\n",
        "\n",
        "Dùng Tensorflow hãy lập trình để tính theta chỉ bằng các phép toán đại số như trên.\n",
        "\n",
        "Gợi ý dùng các hàm:\n",
        "\n",
        "    tf.matmul(A, B): nhân 2 ma trận A và B\n",
        "    \n",
        "    tf.pad(X, [[0,0],[0,1], \"CONSTANT\", constant_values=1.0): thêm một cột toàn số 1 vào X\n",
        "    \n",
        "    tf.linalg.inv(X): tìm ma trận nghịch đảo của X\n",
        "    \n",
        "    tf.transpose(X): tìm ma trận chuyển vị của X\n",
        "    \n",
        "Vẽ kết quả chạy được\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISz_6GKZ05wC",
        "colab_type": "code",
        "outputId": "2fe74591-4ffe-41de-aa04-c595063b3bd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#GRE = theta_1*(TOEFL)+theta_0\n",
        "padding = tf.constant([[0,0],[0,1]])\n",
        "Xbar = tf.pad(X, padding, \"CONSTANT\", constant_values=1)\n",
        "_A = tf.matmul(tf.transpose(Xbar), Xbar)\n",
        "_b = tf.matmul(tf.transpose(Xbar), y)\n",
        "\n",
        "A = tf.cast(_A, tf.float64)\n",
        "b = tf.cast(_b, tf.float64)\n",
        "\n",
        "\n",
        "A_T=tf.linalg.inv(A)\n",
        "theta = tf.matmul(A_T, b)\n",
        "\n",
        "sess = tf.Session()\n",
        "print(sess.run(theta))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  1.5362635 ]\n",
            " [151.79684301]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lG9F4dNTczx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}